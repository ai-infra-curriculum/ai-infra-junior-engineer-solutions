---
# Scenario 06: Liveness and Readiness Probe Issues
#
# Problem: Incorrect probe configuration causing pod restarts or traffic routing issues
# Symptoms: Pods restarting frequently, service not receiving traffic
# Learning Objective: Debug health check configurations

apiVersion: v1
kind: Namespace
metadata:
  name: debug-scenario-06
  labels:
    scenario: probe-issues
    exercise: debugging

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: broken-liveness-app
  namespace: debug-scenario-06
  labels:
    app: broken-liveness
    scenario: probe-issues
spec:
  replicas: 2
  selector:
    matchLabels:
      app: broken-liveness
  template:
    metadata:
      labels:
        app: broken-liveness
    spec:
      containers:
      - name: app
        image: hashicorp/http-echo:latest
        args:
        - "-text=Liveness App"
        - "-listen=:8080"
        ports:
        - containerPort: 8080
          name: http
        livenessProbe:
          httpGet:
            # ISSUE: Wrong path - endpoint doesn't exist
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 1
          failureThreshold: 2  # Will restart pod after 2 failures
        resources:
          requests:
            memory: "32Mi"
            cpu: "50m"
          limits:
            memory: "64Mi"
            cpu: "100m"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slow-startup-app
  namespace: debug-scenario-06
  labels:
    app: slow-startup
    scenario: probe-issues
spec:
  replicas: 1
  selector:
    matchLabels:
      app: slow-startup
  template:
    metadata:
      labels:
        app: slow-startup
    spec:
      containers:
      - name: app
        image: busybox:latest
        command:
        - /bin/sh
        - -c
        - |
          echo "Starting slow application..."
          # Simulate slow startup (60 seconds)
          sleep 60
          echo "Application ready!"
          # Create health endpoint
          mkdir -p /tmp
          echo "OK" > /tmp/health
          # Keep running
          while true; do sleep 30; done
        livenessProbe:
          exec:
            command:
            - cat
            - /tmp/health
          # ISSUE: initialDelaySeconds too short for slow startup
          initialDelaySeconds: 10  # Should be at least 60
          periodSeconds: 5
          timeoutSeconds: 1
          failureThreshold: 3
        resources:
          requests:
            memory: "32Mi"
            cpu: "50m"
          limits:
            memory: "64Mi"
            cpu: "100m"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: failing-readiness-app
  namespace: debug-scenario-06
  labels:
    app: failing-readiness
    scenario: probe-issues
spec:
  replicas: 3
  selector:
    matchLabels:
      app: failing-readiness
  template:
    metadata:
      labels:
        app: failing-readiness
    spec:
      containers:
      - name: app
        image: nginx:1.21-alpine
        ports:
        - containerPort: 80
          name: http
        readinessProbe:
          httpGet:
            # ISSUE: Wrong port - nginx listens on 80, not 8080
            path: /
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 2
          failureThreshold: 3
          successThreshold: 1
        resources:
          requests:
            memory: "32Mi"
            cpu: "50m"
          limits:
            memory: "64Mi"
            cpu: "100m"

---
apiVersion: v1
kind: Service
metadata:
  name: failing-readiness-svc
  namespace: debug-scenario-06
spec:
  selector:
    app: failing-readiness
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: correct-probes-app
  namespace: debug-scenario-06
  labels:
    app: correct-probes
    scenario: probe-issues
spec:
  replicas: 2
  selector:
    matchLabels:
      app: correct-probes
  template:
    metadata:
      labels:
        app: correct-probes
    spec:
      containers:
      - name: app
        image: hashicorp/http-echo:latest
        args:
        - "-text=Healthy App"
        - "-listen=:8080"
        ports:
        - containerPort: 8080
          name: http
        livenessProbe:
          httpGet:
            path: /  # Correct path
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /  # Correct path
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 2
          failureThreshold: 3
          successThreshold: 1
        startupProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 2
          failureThreshold: 30  # Allows up to 150s for startup
        resources:
          requests:
            memory: "32Mi"
            cpu: "50m"
          limits:
            memory: "64Mi"
            cpu: "100m"

---
# Debugging Commands for this scenario:
#
# 1. Check pod status and restarts:
#    kubectl get pods -n debug-scenario-06
#    # Look for high RESTARTS count
#
# 2. Watch pods for restart behavior:
#    kubectl get pods -n debug-scenario-06 -w
#
# 3. Check pod events for probe failures:
#    kubectl describe pod -n debug-scenario-06 <broken-liveness-pod>
#    # Look for "Liveness probe failed" or "Readiness probe failed"
#
# 4. View recent events:
#    kubectl get events -n debug-scenario-06 --sort-by='.lastTimestamp' | grep -i probe
#
# 5. Check service endpoints (readiness affects this):
#    kubectl get endpoints -n debug-scenario-06 failing-readiness-svc
#    # Should show no endpoints if readiness probes are failing
#
# 6. Test probe endpoint manually:
#    kubectl exec -n debug-scenario-06 <pod-name> -- wget -O- http://localhost:8080/healthz
#    # This will fail for broken-liveness-app
#
# 7. Check probe configuration:
#    kubectl get pod -n debug-scenario-06 <pod-name> -o yaml | grep -A 10 livenessProbe
#    kubectl get pod -n debug-scenario-06 <pod-name> -o yaml | grep -A 10 readinessProbe
#
# 8. Fix broken-liveness-app:
#    kubectl patch deployment broken-liveness-app -n debug-scenario-06 --type=json -p='[
#      {
#        "op": "replace",
#        "path": "/spec/template/spec/containers/0/livenessProbe/httpGet/path",
#        "value": "/"
#      }
#    ]'
#
# 9. Fix slow-startup-app:
#    kubectl patch deployment slow-startup-app -n debug-scenario-06 --type=json -p='[
#      {
#        "op": "replace",
#        "path": "/spec/template/spec/containers/0/livenessProbe/initialDelaySeconds",
#        "value": 65
#      }
#    ]'
#
# 10. Fix failing-readiness-app:
#     kubectl patch deployment failing-readiness-app -n debug-scenario-06 --type=json -p='[
#       {
#         "op": "replace",
#         "path": "/spec/template/spec/containers/0/readinessProbe/httpGet/port",
#         "value": 80
#       }
#     ]'
#
# 11. Verify endpoints are now populated:
#     kubectl get endpoints -n debug-scenario-06 failing-readiness-svc
#
# 12. Monitor pod stability:
#     watch kubectl get pods -n debug-scenario-06
#     # Pods should stop restarting
