# Inference Gateway Configuration

# Application
APP_NAME=inference-gateway
APP_VERSION=1.0.0
ENVIRONMENT=development
DEBUG=false

# Server
HOST=0.0.0.0
PORT=8000
WORKERS=4
RELOAD=false

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Model Configuration
MODEL_NAME=resnet50
MODEL_WARMUP=true
MODEL_DEVICE=cpu

# Performance
MAX_QUEUE_SIZE=100
REQUEST_TIMEOUT=30
INFERENCE_TIMEOUT=10

# Observability - Metrics
ENABLE_METRICS=true
METRICS_PORT=8000

# Observability - Tracing
ENABLE_TRACING=true
OTEL_SERVICE_NAME=inference-gateway
OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4318
OTEL_EXPORTER_OTLP_INSECURE=true
OTEL_TRACES_SAMPLER=always_on

# SLO Targets
SLO_AVAILABILITY_TARGET=99.5
SLO_LATENCY_P99_TARGET_MS=300
SLO_LATENCY_P95_TARGET_MS=200

# CORS
CORS_ORIGINS=*
CORS_ALLOW_CREDENTIALS=true
CORS_ALLOW_METHODS=*
CORS_ALLOW_HEADERS=*
