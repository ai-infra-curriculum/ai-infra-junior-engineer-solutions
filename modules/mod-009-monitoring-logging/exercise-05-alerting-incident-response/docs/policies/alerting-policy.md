# Alerting Policy

**Version**: 1.0
**Last Updated**: 2025-10-23
**Owner**: SRE Team
**Reviewers**: ML Platform Team, Engineering Leadership
**Next Review**: 2025-11-23

---

## Table of Contents

1. [Overview](#overview)
2. [Alert Severity Levels](#alert-severity-levels)
3. [Alert Design Principles](#alert-design-principles)
4. [On-Call Response SLAs](#on-call-response-slas)
5. [Alert Routing and Escalation](#alert-routing-and-escalation)
6. [Alert Lifecycle](#alert-lifecycle)
7. [Alert Quality Metrics](#alert-quality-metrics)
8. [Alert Review Process](#alert-review-process)

---

## Overview

This document defines the alerting policy for the ML infrastructure platform, establishing standards for alert creation, severity classification, routing, and response expectations.

### Goals

1. **Minimize MTTR** (Mean Time To Resolution): Alerts provide actionable information
2. **Reduce Alert Fatigue**: High signal-to-noise ratio, minimal false positives
3. **Prevent Incidents**: Early warning before user impact
4. **Optimize Response**: Right person, right time, right information

### Scope

This policy applies to all alerts generated by:
- Prometheus Alertmanager
- Application-level alerts
- Infrastructure monitoring
- Security monitoring
- SLO-based alerts

---

## Alert Severity Levels

### P0 - Critical (Page Immediately)

**Characteristics**:
- **User Impact**: Severe, affecting majority of users or critical functionality
- **Service Status**: Complete outage or severe degradation
- **Business Impact**: Revenue loss, contractual breach, security incident
- **Response Time**: <2 minutes acknowledgment, <15 minutes MTTR goal

**Examples**:
- `ServiceDown`: Inference gateway completely unreachable
- `SLOAvailabilityFastBurn`: Error budget exhaustion imminent (<2 hours)
- `DatabaseDown`: Primary database unreachable
- `SecurityBreach`: Active security incident detected

**Notification Channels**:
- PagerDuty (immediate page)
- Slack #incidents channel
- SMS to on-call engineer
- Auto-escalate if not acknowledged in 5 minutes

**Escalation Path**: On-Call Engineer → Service Owner → Engineering Manager → VP Engineering (15-minute intervals)

---

### P1 - High (Page During Business Hours)

**Characteristics**:
- **User Impact**: Moderate, affecting subset of users or degraded experience
- **Service Status**: Partial degradation, elevated error rate
- **Business Impact**: Potential revenue impact, SLO at risk
- **Response Time**: <5 minutes acknowledgment, <30 minutes MTTR goal

**Examples**:
- `HighErrorRate`: >5% error rate for 5 minutes
- `HighLatency`: P99 latency >1 second
- `HighMemoryUsage`: Memory usage >90% for 10 minutes
- `SLOAvailabilitySlowBurn`: Error budget burning at 6x rate

**Notification Channels**:
- PagerDuty (page during business hours 9am-9pm)
- Slack #incidents channel
- Email to on-call engineer

**Escalation Path**: On-Call Engineer → Service Owner → SRE Lead (30-minute intervals)

---

### P2 - Medium (Notify, No Page)

**Characteristics**:
- **User Impact**: Minor or no user impact yet
- **Service Status**: Warning signs, degrading but stable
- **Business Impact**: Potential future impact if unaddressed
- **Response Time**: <30 minutes acknowledgment, <2 hours resolution goal

**Examples**:
- `HighCPUUsage`: CPU usage >80% for 15 minutes
- `HighMemoryUsage`: Memory usage >85% for 15 minutes
- `DiskSpaceWarning`: Disk usage >85%
- `LatencyWarning`: P99 latency >500ms

**Notification Channels**:
- Slack #monitoring channel
- Email to team distribution list
- Ticket created in issue tracking system

**Escalation Path**: Team → Service Owner (next business day if no response)

---

### P3 - Low (Informational)

**Characteristics**:
- **User Impact**: None
- **Service Status**: Informational, FYI
- **Business Impact**: None
- **Response Time**: Best effort, review during business hours

**Examples**:
- `HighRequestRate`: Traffic spike (informational)
- `DeploymentSuccessful`: Deployment completed
- `CertificateExpiringSoon`: Certificate expires in 30 days
- `BackupCompleted`: Scheduled backup finished

**Notification Channels**:
- Slack #monitoring channel (quiet hours respected)
- Daily digest email
- Dashboard annotations

**Escalation Path**: None (no action required)

---

## Alert Design Principles

### 1. Symptom-Based Alerts (Preferred)

Alert on **user-visible symptoms**, not internal component states.

**Good** (Symptom):
```yaml
- alert: HighErrorRate
  expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
  annotations:
    summary: "Users experiencing elevated error rate"
```

**Bad** (Cause):
```yaml
- alert: ProcessCrashed
  expr: up{job="api-server"} == 0
  annotations:
    summary: "API server process is down"
```

**Why**: Process might auto-restart with no user impact. Alert on actual user impact (error rate).

---

### 2. Actionable Alerts

Every alert must have a **clear runbook** with specific remediation steps.

**Required in alert annotations**:
```yaml
annotations:
  summary: "Brief description of problem"
  description: "Detailed context with current values"
  runbook_url: "https://runbooks.company.com/high-error-rate"
  dashboard: "https://grafana.company.com/d/app-performance"
```

**Runbook must include**:
- Triage steps (first 5 minutes)
- Investigation procedure
- Common root causes
- Mitigation actions
- Communication templates

---

### 3. Multi-Window Multi-Burn-Rate (MWMBR)

For SLO-based alerts, use **multiple time windows** to reduce false positives.

**Example**:
```yaml
- alert: SLOAvailabilityFastBurn
  expr: |
    slo:availability:burn_rate:1h > 14.4
    and
    slo:availability:burn_rate:6h > 14.4
  for: 2m
```

**Benefits**:
- 1-hour window catches recent changes
- 6-hour window filters temporary blips
- Both must trigger → 90% fewer false positives

---

### 4. Appropriate Thresholds

Set thresholds based on **historical data and SLOs**, not arbitrary values.

**Process**:
1. Analyze historical metrics (P50, P90, P99)
2. Set warning threshold at P90 of normal behavior
3. Set critical threshold at SLO breach point
4. Add `for` duration to avoid alert flapping

**Example**:
```yaml
# Historical P99 latency: 150ms
# SLO target: 300ms
# Warning: 200ms (early warning)
# Critical: 500ms (SLO breach imminent)

- alert: LatencyWarning
  expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 0.2
  for: 10m  # Must persist 10 minutes to avoid flapping

- alert: LatencyCritical
  expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 0.5
  for: 5m
```

---

### 5. Alert Naming Conventions

Use **consistent naming** for easy identification:

**Format**: `<Severity><Component><Symptom>`

**Examples**:
- `HighErrorRate` (not "APIErrors" or "TooManyFailures")
- `ServiceDown` (not "TargetUnreachable" or "HealthCheckFailed")
- `HighMemoryUsage` (not "MemoryProblem" or "OOMRisk")

**Rules**:
- Start with severity: High, Low, Critical
- Component: Service name or resource type
- Symptom: User-visible problem

---

## On-Call Response SLAs

### Acknowledgment SLAs

| Severity | Acknowledgment Time | MTTR Goal |
|----------|---------------------|-----------|
| P0 - Critical | 2 minutes | 15 minutes |
| P1 - High | 5 minutes | 30 minutes |
| P2 - Medium | 30 minutes | 2 hours |
| P3 - Low | Best effort | N/A |

### Response Actions

**Acknowledge** (all severities):
- Click "Acknowledge" in PagerDuty
- Post in Slack that you're investigating
- Open incident dashboard

**Declare Incident** (P0, P1 only):
- Post in #incidents channel with incident template
- Assign Incident Commander (IC)
- Open war room (Zoom/Slack bridge)

**Communicate** (P0, P1 only):
- Status updates every 10 minutes (P0) or 30 minutes (P1)
- Post resolution message when resolved
- Schedule post-mortem within 48 hours

---

## Alert Routing and Escalation

### Routing Configuration (Alertmanager)

```yaml
route:
  group_by: ['alertname', 'service']
  group_wait: 10s         # Wait to batch similar alerts
  group_interval: 10s     # Interval between batches
  repeat_interval: 4h     # Re-send if not resolved
  receiver: 'default'

  routes:
    # Critical alerts → page immediately
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      continue: true  # Also send to Slack

    # High alerts → page during business hours
    - match:
        severity: high
      receiver: 'pagerduty-high'
      continue: true

    # Medium alerts → Slack only
    - match:
        severity: warning
      receiver: 'slack-monitoring'

    # Low alerts → email digest
    - match:
        severity: info
      receiver: 'email-daily-digest'
```

### Escalation Policy

**P0 - Critical**:
1. **0 min**: Primary on-call (PagerDuty)
2. **5 min**: Secondary on-call (if not acknowledged)
3. **15 min**: Service owner + SRE lead
4. **30 min**: Engineering manager + VP Engineering
5. **60 min**: CTO notification

**P1 - High**:
1. **0 min**: Primary on-call
2. **15 min**: Secondary on-call
3. **30 min**: Service owner
4. **60 min**: SRE lead

**P2 - Medium**:
1. **0 min**: Team Slack channel
2. **Next business day**: Service owner review

---

## Alert Lifecycle

### 1. Alert Fires

**Automatic actions**:
- Notification sent via configured channels
- Dashboard annotation created
- Incident ticket created (P0, P1)
- Escalation timer starts

**On-call engineer actions**:
- Acknowledge alert
- Open runbook
- Post in #incidents (P0, P1) or #monitoring (P2)

---

### 2. Alert Investigation

**On-call engineer actions**:
- Follow runbook procedures
- Check dashboards and logs
- Identify root cause
- Document findings in incident thread

**Status updates**:
- P0: Every 10 minutes
- P1: Every 30 minutes
- P2: At resolution

---

### 3. Alert Mitigation

**On-call engineer actions**:
- Apply mitigation from runbook
- Verify metrics improving
- Monitor for stability (10-15 minutes)
- Post mitigation actions in incident thread

---

### 4. Alert Resolution

**On-call engineer actions**:
- Verify alert auto-resolved or manually resolve
- Post resolution message with:
  - Root cause summary
  - Actions taken
  - Duration
  - SLO impact
  - Next steps (post-mortem, follow-up items)

**Automatic actions**:
- Close PagerDuty incident
- Update incident ticket
- Generate incident report template

---

### 5. Post-Incident Review

**Required for**:
- All P0 incidents
- P1 incidents with user impact
- Any incident exceeding MTTR goal

**Timeline**:
- Post-mortem document: Within 24 hours
- Post-mortem meeting: Within 48 hours

**See**: `docs/policies/incident-response-policy.md`

---

## Alert Quality Metrics

### Key Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Precision** (true positives) | >95% | (True alerts / Total alerts) × 100 |
| **Time to Acknowledge** | <SLA | P50, P95, P99 by severity |
| **Time to Resolution** | <MTTR goal | P50, P95, P99 by severity |
| **Alert Volume** | <50/day | Total alerts per day |
| **Escalation Rate** | <10% | Escalations / Total alerts |

### Alert Quality Review

**Monthly review** of each alert:
- Fire count: How many times this month?
- False positive rate: How many were not real issues?
- MTTR: Average resolution time
- Runbook usage: Was runbook followed? Was it helpful?

**Action items**:
- **>20 fires/month**: Increase threshold or add longer `for` duration
- **>10% false positives**: Refine query or add multi-window validation
- **MTTR >2x goal**: Improve runbook or add automation
- **Never fired**: Consider removing (dead alert)

---

## Alert Review Process

### Weekly Alert Review (Team)

**Attendees**: On-call engineers, service owners
**Duration**: 30 minutes
**Agenda**:
1. Review alerts from past week
2. Identify patterns (same alert multiple times)
3. Discuss false positives
4. Update runbooks based on learnings

---

### Monthly Alert Audit (SRE Team)

**Process**:
1. Export alert firing data from Prometheus
2. Calculate quality metrics
3. Identify top 10 noisy alerts
4. Identify alerts that never fired
5. Create remediation tickets

**Output**: Alert quality report with action items

---

### Quarterly Alert Strategy Review (Leadership)

**Attendees**: SRE lead, service owners, engineering manager
**Duration**: 1 hour
**Agenda**:
1. Review alert quality trends
2. Assess SLO coverage (are we alerting on the right things?)
3. Identify gaps (missing alerts for known failure modes)
4. Prioritize improvements

---

## Alert Anti-Patterns

### ❌ Don't: Alert on Everything

**Bad**:
```yaml
- alert: CPUUsageAbove10Percent
  expr: cpu_usage > 0.1
```

**Why**: Normal operation, creates noise

---

### ❌ Don't: Alert Without Runbook

**Bad**: Alert without clear action
**Fix**: Every alert must have a runbook URL

---

### ❌ Don't: Set Arbitrary Thresholds

**Bad**: `cpu_usage > 80%` without data analysis
**Fix**: Analyze historical data, set thresholds based on P90/P95

---

### ❌ Don't: Alert on Causes Instead of Symptoms

**Bad**: `ProcessDown` (cause)
**Fix**: `HighErrorRate` (symptom users experience)

---

### ❌ Don't: Page for Non-Urgent Issues

**Bad**: Paging for warnings that don't need immediate action
**Fix**: Use appropriate severity (P2/P3 for warnings)

---

## Alert Best Practices

### ✅ Do: Use SLO-Based Alerts

Alert on SLO burn rate, not arbitrary thresholds

### ✅ Do: Include Context in Annotations

Provide query results, dashboard links, affected resources

### ✅ Do: Test Alerts

Trigger alerts in staging to verify routing and runbooks

### ✅ Do: Keep Runbooks Updated

Update after each incident based on what worked

### ✅ Do: Review Alert Quality Regularly

Monthly review of false positives and MTTR

---

## Appendix A: Alert Template

```yaml
groups:
  - name: application_alerts
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{service="inference-gateway", status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{service="inference-gateway"}[5m]))
          ) * 100 > 5
        for: 2m
        labels:
          severity: critical
          service: inference-gateway
          team: ml-platform
        annotations:
          summary: "High error rate on {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
          runbook_url: "https://runbooks.company.com/high-error-rate"
          dashboard: "http://grafana.company.com/d/app-performance?var-service={{ $labels.service }}"
          query: "sum(rate(http_requests_total{service=\"{{ $labels.service }}\", status=~\"5..\"}[5m]))"
```

---

## Appendix B: Notification Templates

### Slack Template (Critical)

```
🔴 CRITICAL ALERT: {{ .GroupLabels.alertname }}

Service: {{ .GroupLabels.service }}
Severity: {{ .GroupLabels.severity }}
Fired: {{ .StartsAt | humanizeTimestamp }}

{{ range .Alerts }}
• {{ .Annotations.summary }}
  Value: {{ .Annotations.description }}
  Runbook: {{ .Annotations.runbook_url }}
  Dashboard: {{ .Annotations.dashboard }}
{{ end }}

@oncall Please acknowledge immediately!
```

### Email Template (Daily Digest)

```
Subject: [Monitoring] Daily Alert Digest - {{ .Date }}

Summary:
- Critical alerts: {{ .CriticalCount }}
- High alerts: {{ .HighCount }}
- Warning alerts: {{ .WarningCount }}

Top 5 Alerts:
{{ range .TopAlerts }}
1. {{ .Name }} (fired {{ .Count }} times)
   Last: {{ .LastFired }}
   Service: {{ .Service }}
{{ end }}

View full report: https://monitoring.company.com/digest/{{ .Date }}
```

---

## Document Change Log

| Date | Version | Author | Changes |
|------|---------|--------|---------|
| 2025-10-23 | 1.0 | SRE Team | Initial policy creation |

---

## Approval

**Policy Owner**: SRE Team Lead
**Approved By**: VP of Engineering
**Effective Date**: 2025-10-23
**Next Review**: 2025-11-23
